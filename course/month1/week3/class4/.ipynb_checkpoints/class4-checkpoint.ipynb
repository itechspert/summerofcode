{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk, matplotlib, re, pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176965"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = 'http://www.gutenberg.org/files/2554/2554-0.txt'\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " '.',\n",
       " 'You',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it',\n",
       " ',',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or',\n",
       " 're-use',\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'License']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Optional:\n",
    "Write your own my_tokenize() function that takes in a string and returns a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "# we 'cast' the previous list into a new format.\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Regular expressions for pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['math', 'oath']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = (w for w in nltk.corpus.words.words('en') if w.islower())\n",
    "[w for w in wordlist if re.search('^..c.ing', w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adman',\n",
       " 'almon',\n",
       " 'amman',\n",
       " 'atman',\n",
       " 'axman',\n",
       " 'beman',\n",
       " 'caman',\n",
       " 'cumin',\n",
       " 'daman',\n",
       " 'demon',\n",
       " 'gamin',\n",
       " 'hemen',\n",
       " 'hemin',\n",
       " 'human',\n",
       " 'humin',\n",
       " 'hymen',\n",
       " 'jaman',\n",
       " 'lamin',\n",
       " 'leman',\n",
       " 'lemon',\n",
       " 'liman',\n",
       " 'limen',\n",
       " 'lumen',\n",
       " 'numen',\n",
       " 'osmin',\n",
       " 'oxman',\n",
       " 'reman',\n",
       " 'rumen',\n",
       " 'saman',\n",
       " 'samen',\n",
       " 'semen',\n",
       " 'teman',\n",
       " 'temin',\n",
       " 'timon',\n",
       " 'toman',\n",
       " 'tomin',\n",
       " 'ulmin',\n",
       " 'unman',\n",
       " 'urman',\n",
       " 'vimen',\n",
       " 'woman',\n",
       " 'yamen',\n",
       " 'zaman',\n",
       " 'zymin']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = (w for w in nltk.corpus.words.words('en') if w.islower())\n",
    "[w for w in wordlist if re.search('^..m.n$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['math', 'oath']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = (w for w in nltk.corpus.words.words('en') if w.islower())\n",
    "#6284\n",
    "[w for w in wordlist if re.search('^[mno][abc][tuv][ghi]$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = (w for w in nltk.corpus.words.words('en') if w.islower())\n",
    "#here w is used to iterate through nltk corpus. List without square-brackets. Because it's in function, it's not complaining.\n",
    "\n",
    "chat_words = sorted(set(w\n",
    "        for w in nltk.corpus.nps_chat.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
    "#We could call this completely differently, almost like w2. For every word, we want to do this check for that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ae']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in chat_words if re.search('^m*i*(ae)+n*e*$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[^aeiouAEIOU] -> #The meaning changes to not xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework \n",
    "Priorities for the day. \n",
    "- Read about file inputs and outputs (reading from files).\n",
    "- Regular expressions (regex). Look at the table and read about what Regex actualy does. \n",
    "- Additional homework (the important stuff):\n",
    "- Exercises 6-10\n",
    "If you are short on time you can skip, or do these later:\n",
    "- Unicode encoding\n",
    "☼ Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "[a-zA-Z]+\n",
    "[A-Z][a-z]*\n",
    "p[aeiou]{,2}t\n",
    "\\d+(\\.\\d+)?\n",
    "([^aeiou][aeiou][^aeiou])*\n",
    "\\w+|[^\\w\\s]+\n",
    "Test your answers using nltk.re_show().\n",
    "\n",
    "☼ Write regular expressions to match the following classes of strings:\n",
    "\n",
    "A single determiner (assume that a, an, and the are the only determiners).\n",
    "An arithmetic expression using integers, addition, and multiplication, such as 2*3+8.\n",
    "☼ Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then  request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL.\n",
    "\n",
    "☼ Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x).\n",
    "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations.\n",
    "☼ Rewrite the following loop as a list comprehension:\n",
    "\n",
    "'''  #Check this in the original code format.\t\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "...     word_len = (word, len(word))\n",
    "...     result.append(word_len)\n",
    " result\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
